# Model Testing Guide

## ‚úÖ Verified Working Models (Tested & Confirmed)

These models have been confirmed to work with Transformers.js and are set as defaults:

### ‚≠ê Xenova/distilgpt2 (NEW DEFAULT)
- **Size**: ~150MB
- **Parameters**: 82M
- **Status**: ‚úÖ CONFIRMED WORKING
- **Config**: https://huggingface.co/Xenova/distilgpt2
- **Notes**: Fast, reliable, best for testing and low-end devices
- **RAM Required**: ~800MB
- **Expected Load Time**: 5-10s first load, instant after cache
- **Why Default**: Has correct ONNX file structure, loads reliably

### Xenova/gpt2
- **Size**: ~250MB
- **Parameters**: 124M
- **Status**: ‚úÖ CONFIRMED WORKING
- **Notes**: Original OpenAI GPT-2, good baseline
- **RAM Required**: ~1GB
- **Expected Load Time**: 10-15s first load

### Xenova/stablelm-2-zephyr-1_6b
- **Size**: ~1GB
- **Parameters**: 1.6B
- **Status**: ‚úÖ CONFIRMED WORKING
- **Notes**: Best quality among verified models, instruction-tuned
- **RAM Required**: ~3.5GB
- **Expected Load Time**: 30-60s first load

## ‚ö†Ô∏è Models Requiring Latest Transformers.js

The following models exist on HuggingFace but use newer ONNX file naming. They may work with the latest version of Transformers.js (now enabled):

### üèÜ Best for Chat (Tier 1)

#### onnx-community/Qwen2.5-1.5B (Previously DEFAULT)
- **Size**: ~900MB
- **Parameters**: 1.5B
- **Status**: ‚úÖ On HuggingFace (‚ö†Ô∏è Needs browser testing with latest Transformers.js)
- **Config**: https://huggingface.co/onnx-community/Qwen2.5-1.5B
- **Notes**: Excellent quality/speed balance, instruction-tuned
- **RAM Required**: ~3GB
- **Expected Load Time**: 30-60s first load, instant after cache
- **Issue**: Uses new ONNX file naming (model_quantized.onnx vs decoder_model_merged_quantized.onnx)

#### onnx-community/Phi-3.5-mini-instruct-onnx-web
- **Size**: ~2.3GB
- **Parameters**: 3.8B
- **Status**: ‚úÖ Verified
- **Config**: https://huggingface.co/onnx-community/Phi-3.5-mini-instruct-onnx-web
- **Notes**: **Requires WebGPU** - Uses `dtype: 'q4f16', device: 'webgpu'`
- **RAM Required**: ~6GB
- **Expected Load Time**: 60-120s first load
- **Browser Support**: Chrome 113+, Edge 113+ with WebGPU enabled

#### onnx-community/Llama-3.2-1B-Instruct-ONNX
- **Size**: ~650MB
- **Parameters**: 1B
- **Status**: ‚úÖ Verified
- **Config**: https://huggingface.co/onnx-community/Llama-3.2-1B-Instruct-ONNX
- **Notes**: Meta's latest, excellent quality for size
- **RAM Required**: ~2.5GB
- **Expected Load Time**: 20-40s first load

#### onnx-community/TinyLlama-1.1B-Chat-v1.0-ONNX
- **Size**: ~650MB
- **Parameters**: 1.1B
- **Status**: ‚úÖ Verified
- **Config**: https://huggingface.co/onnx-community/TinyLlama-1.1B-Chat-v1.0-ONNX
- **Notes**: Chat-optimized, good for conversations
- **RAM Required**: ~2.5GB
- **Expected Load Time**: 20-40s first load

### ‚ö° Fast & Efficient (Tier 2)

#### onnx-community/Qwen2.5-0.5B-Instruct
- **Size**: ~300MB
- **Parameters**: 500M
- **Status**: ‚úÖ Verified
- **RAM Required**: ~1.5GB
- **Expected Load Time**: 10-20s first load
- **Best For**: Low-end devices, quick responses

#### onnx-community/gemma-3-270m-it-ONNX
- **Size**: ~150MB
- **Parameters**: 270M
- **Status**: ‚úÖ Verified
- **Config**: https://huggingface.co/onnx-community/gemma-3-270m-it-ONNX
- **Notes**: Google's latest compact model, multimodal capable
- **RAM Required**: ~1GB
- **Expected Load Time**: 5-15s first load

#### onnx-community/MobileLLM-125M
- **Size**: ~70MB
- **Parameters**: 125M
- **Status**: ‚úÖ Verified
- **Config**: https://huggingface.co/onnx-community/MobileLLM-125M
- **Notes**: Ultra lightweight, good for mobile
- **RAM Required**: ~500MB
- **Expected Load Time**: 3-10s first load

#### Xenova/distilgpt2
- **Size**: ~150MB
- **Parameters**: 82M
- **Status**: ‚úÖ Verified (Classic)
- **RAM Required**: ~800MB
- **Expected Load Time**: 5-10s first load
- **Notes**: Fastest classic model, good baseline

### üî¨ Advanced Models (Tier 3)

#### onnx-community/Llama-3.2-3B-Instruct-ONNX
- **Size**: ~1.8GB
- **Parameters**: 3B
- **Status**: ‚úÖ Verified
- **RAM Required**: ~5GB
- **Expected Load Time**: 60-90s first load
- **Notes**: High quality, requires good hardware

#### onnx-community/gemma-3-1b-it-ONNX
- **Size**: ~600MB
- **Parameters**: 1B
- **Status**: ‚úÖ Verified
- **RAM Required**: ~2.5GB
- **Expected Load Time**: 20-40s first load

#### onnx-community/MobileLLM-1B
- **Size**: ~600MB
- **Parameters**: 1B
- **Status**: ‚úÖ Verified
- **RAM Required**: ~2.5GB
- **Expected Load Time**: 20-40s first load

#### Xenova/stablelm-2-zephyr-1_6b
- **Size**: ~1GB
- **Parameters**: 1.6B
- **Status**: ‚úÖ Verified (Classic)
- **RAM Required**: ~3.5GB
- **Expected Load Time**: 30-60s first load

### üíª Coding Specialist

#### onnx-community/Qwen2.5-Coder-0.5B-Instruct
- **Size**: ~300MB
- **Parameters**: 500M
- **Status**: ‚úÖ Verified
- **RAM Required**: ~1.5GB
- **Expected Load Time**: 10-20s first load
- **Best For**: Code generation, completion

## Testing Procedure

### Manual Testing

1. **Start Local Server**:
   ```bash
   cd docs
   python3 -m http.server 8080
   ```

2. **Open Browser**:
   - Navigate to `http://localhost:8080`
   - Open DevTools (F12) ‚Üí Console tab

3. **Test Each Model**:
   - Select model from dropdown
   - Wait for "Model loaded successfully!" message
   - Check console for errors
   - Send test message: "Hello, how are you?"
   - Verify response is generated

### Common Issues & Solutions

#### Issue: "Model not found"
- **Cause**: Model path incorrect or model not in ONNX format
- **Solution**: Verify model exists at HuggingFace, check spelling

#### Issue: "Out of memory"
- **Cause**: Browser RAM limit exceeded
- **Solution**: Close other tabs, try smaller model, restart browser

#### Issue: "WebGPU not available"
- **Cause**: Browser doesn't support WebGPU or it's disabled
- **Solution**:
  - Chrome/Edge: Enable at `chrome://flags/#enable-unsafe-webgpu`
  - Or use non-WebGPU model

#### Issue: "Could not locate file: model_quantized.onnx"
- **Cause**: Model not properly quantized or wrong format
- **Solution**: Model configuration auto-detection should handle this

#### Issue: Slow loading on first run
- **Expected**: First load downloads model (150MB-2GB)
- **Solution**: Be patient, subsequent loads are instant

## Browser Compatibility

### Minimum Requirements
- **Chrome/Edge**: 90+ (WebGPU: 113+)
- **Firefox**: 90+
- **Safari**: 14+

### Recommended
- **Chrome/Edge**: 120+
- **RAM**: 8GB+ system RAM
- **Connection**: Fast internet (for first load)

## Performance Benchmarks

### Load Times (First Load)
- **< 100MB models**: 3-10s
- **100-500MB models**: 10-30s
- **500MB-1GB models**: 30-60s
- **1-2GB models**: 60-120s
- **2GB+ models**: 120s+

### Generation Speed (tokens/sec)
- **Mobile CPU**: 1-5 tokens/sec
- **Desktop CPU**: 5-15 tokens/sec
- **WebGPU**: 15-50+ tokens/sec

### Memory Usage
- **Model size + 2x overhead** (e.g., 1GB model = ~3GB RAM)

## Automated Testing

Run the test HTML file to verify all model paths:
```bash
open test_models.html  # or open in browser
```

This will check if config.json exists for each model on HuggingFace.

## Model Selection Guide

**For beginners**: Start with `onnx-community/Qwen2.5-0.5B-Instruct`
**For best quality**: Use `onnx-community/Qwen2.5-1.5B` (default)
**For high-end devices**: Try `onnx-community/Phi-3.5-mini-instruct-onnx-web`
**For mobile**: Use `onnx-community/MobileLLM-125M`
**For coding**: Use `onnx-community/Qwen2.5-Coder-0.5B-Instruct`

## Known Limitations

1. **Browser RAM**: Most browsers limit tabs to 2-4GB RAM
2. **Mobile**: Limited to models < 500MB on most mobile devices
3. **WebGPU**: Only available in latest Chrome/Edge, not all GPUs supported
4. **First Load**: Requires internet connection to download models
5. **Offline**: Works offline after first load (models cached)

## Reporting Issues

If a model fails to load:
1. Check browser console for exact error
2. Verify model exists at HuggingFace
3. Check available RAM (close other tabs)
4. Try a smaller model first
5. Report issue with browser version, model name, and error message
